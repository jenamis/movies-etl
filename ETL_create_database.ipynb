{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clean movie function that takes in argument \"movie\"\n",
    "def clean_movie(movie):\n",
    "    \n",
    "    # Create a non-destructive copy\n",
    "    movie = dict(movie)\n",
    "\n",
    "    # Combine alternate titles into one list\n",
    "    alt_titles = {}\n",
    "    for key in [\"Also known as\",\"Arabic\",\"Cantonese\",\"Chinese\",\"French\",\n",
    "                \"Hangul\",\"Hebrew\",\"Hepburn\",\"Japanese\",\"Literally\",\n",
    "                \"Mandarin\",\"McCune–Reischauer\",\"Original title\",\"Polish\",\n",
    "                \"Revised Romanization\",\"Romanized\",\"Russian\",\n",
    "                \"Simplified\",\"Traditional\",\"Yiddish\"]:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    if len(alt_titles) > 0:\n",
    "        movie[\"alt_titles\"] = alt_titles\n",
    "    \n",
    "    # Merge column names\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name(\"Adaptation by\", \"Writer(s)\")\n",
    "    change_column_name(\"Country of origin\", \"Country\")\n",
    "    change_column_name(\"Directed by\", \"Director\")\n",
    "    change_column_name(\"Distributed by\", \"Distributor\")\n",
    "    change_column_name(\"Edited by\", \"Editor(s)\")\n",
    "    change_column_name(\"Length\", \"Running time\")\n",
    "    change_column_name(\"Original release\", \"Release date\")\n",
    "    change_column_name(\"Music by\", \"Composer(s)\")\n",
    "    change_column_name(\"Produced by\", \"Producer(s)\")\n",
    "    change_column_name(\"Producer\", \"Producer(s)\")\n",
    "    change_column_name(\"Productioncompanies \", \"Production company(s)\")\n",
    "    change_column_name(\"Productioncompany \", \"Production company(s)\")\n",
    "    change_column_name(\"Released\", \"Release Date\")\n",
    "    change_column_name(\"Release Date\", \"Release date\")\n",
    "    change_column_name(\"Screen story by\", \"Writer(s)\")\n",
    "    change_column_name(\"Screenplay by\", \"Writer(s)\")\n",
    "    change_column_name(\"Story by\", \"Writer(s)\")\n",
    "    change_column_name(\"Theme music composer\", \"Composer(s)\")\n",
    "    change_column_name(\"Written by\", \"Writer(s)\")   \n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that reads in, cleans, and loads data from three files containing Wikipedia data, Kaggle metadata, and MovieLens rating data\n",
    "def extract_transform_load(wiki_data, kaggle_data, ratings_data):\n",
    "    \n",
    "    # Read in Kaggle metadata and MovieLens ratings CSV files as Pandas DFs\n",
    "    kaggle_metadata = pd.read_csv(kaggle_data, low_memory = False)\n",
    "    ratings = pd.read_csv(ratings_data)\n",
    "\n",
    "    # Open and read Wikipedia data JSON file\n",
    "    with open(wiki_data, mode = \"r\") as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # Clean Wikipedia data\n",
    "    # Use list comprehension to filter out TV shows and keep only movies with director and IMDb link\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw \n",
    "                   if \"No. of episodes\" not in movie\n",
    "                   and (\"Director\" in movie or \"Directed by\" in movie) \n",
    "                   and \"imdb_link\" in movie]\n",
    "\n",
    "    # Use list comprehension to iterate through cleaned wiki movies list and call clean_movie function on each movie\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "    # Read in cleaned movies list as DF\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "\n",
    "    # Use try-except block to catch errors while extracting IMDb ID using regex string and dropping any imdb_id duplicates\n",
    "    try:\n",
    "        wiki_movies_df[\"imdb_id\"] = wiki_movies_df[\"imdb_link\"].str.extract(r\"(tt\\d{7})\")\n",
    "        wiki_movies_df.drop_duplicates(subset = \"imdb_id\", inplace = True)\n",
    "        \n",
    "    # If there is an error, capture and print exception\n",
    "    except Exception as err:\n",
    "        print(f\"Error occurred, {err=}\")\n",
    "\n",
    "    # Use list comprehension to keep columns that have less than 90% null values from wiki_movies_df\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "    # Clean box office column in wiki_movies_df\n",
    "    # Create variable to hold non-null values from box office column\n",
    "    box_office = wiki_movies_df[\"Box office\"].dropna()\n",
    "    \n",
    "    # Use lambda and join functions to convert box office data to string values\n",
    "    box_office = box_office.apply(lambda x: \" \".join(x) if type(x) == list else x)\n",
    "\n",
    "    # Remove values between dollar sign and hyphen for box office data given as ranges\n",
    "    box_office = box_office.str.replace(r\"\\$.*[-—–](?![a-z])\", \"$\", regex = True)\n",
    "    \n",
    "    # Use regex to match \"form_one\" of box office data\n",
    "    form_one = r\"\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on\"\n",
    "   \n",
    "    # Use regex to match \"form_two\" of box office data\n",
    "    form_two = r\"\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)\"\n",
    "\n",
    "    # Use function to turn extracted values into numeric values\n",
    "    def parse_dollars(s):\n",
    "    \n",
    "        # If s is not string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "    \n",
    "        # If input is of form $###.# million\n",
    "        if re.match(r\"\\$\\s*\\d+\\.?\\d*\\s*milli?on\", s, flags = re.IGNORECASE):\n",
    "        \n",
    "            # Remove dollar sign and \" million\"\n",
    "            s = re.sub(\"\\$|\\s|[a-zA-Z]\",\"\", s)\n",
    "        \n",
    "            # Convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # Return value\n",
    "            return value\n",
    "\n",
    "        # If input is of form $###.# billion\n",
    "        elif re.match(r\"\\$\\s*\\d+\\.?\\d*\\s*billi?on\", s, flags = re.IGNORECASE):\n",
    "\n",
    "            # Remove dollar sign and \" billion\"\n",
    "            s = re.sub(\"\\$|\\s|[a-zA-Z]\",\"\", s)\n",
    "\n",
    "            # Convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # Return value\n",
    "            return value\n",
    "\n",
    "        # If input is of the form $###,###,###\n",
    "        elif re.match(r\"\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)\", s, flags = re.IGNORECASE):\n",
    "    \n",
    "            # Remove dollar sign and commas\n",
    "            s = re.sub(\"\\$|,\",\"\", s)\n",
    "\n",
    "            # Convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # Return value\n",
    "            return value\n",
    "\n",
    "        # Otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # Create new clean box office column\n",
    "    wiki_movies_df[\"box_office\"] = box_office.str.extract(f\"({form_one}|{form_two})\", flags = re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # Drop original box office column\n",
    "    wiki_movies_df.drop(\"Box office\", axis = 1, inplace = True)\n",
    "    \n",
    "    # Clean budget column in wiki_movies_df\n",
    "    # Create variable to hold non-null values from budget column\n",
    "    budget = wiki_movies_df[\"Budget\"].dropna()\n",
    "    \n",
    "    # Use lambda and join functions to convert budget data to string values\n",
    "    budget = budget.apply(lambda x: \" \".join(x) if type(x) == list else x)\n",
    "\n",
    "    # Remove values between dollar sign and hyphen for budget data given as ranges\n",
    "    budget = budget.str.replace(r\"\\$.*[-—–](?![a-z])\", \"$\", regex = True)    \n",
    "\n",
    "    # Remove citation references from budget data\n",
    "    budget = budget.str.replace(r\"\\[\\d+\\]\\s*\", \"\", regex = True)\n",
    "    \n",
    "    # Create new clean budget column\n",
    "    wiki_movies_df[\"budget\"] = budget.str.extract(f\"({form_one}|{form_two})\", flags = re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # Drop original budget column\n",
    "    wiki_movies_df.drop(\"Budget\", axis = 1, inplace = True)\n",
    "    \n",
    "    # Clean release date column in wiki_movies_df\n",
    "    # Make variable to hold non-null release date values and convert lists to strings\n",
    "    release_date = wiki_movies_df[\"Release date\"].dropna().apply(lambda x: \" \".join(x) if type(x) == list else x)\n",
    "\n",
    "    # Create regex for parsing release date\n",
    "    date_form_one = r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}\"\n",
    "    date_form_two = r\"\\d{4}.[01]\\d.[0123]\\d\"\n",
    "    date_form_three = r\"(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}\"\n",
    "    date_form_four = r\"\\d{4}\"\n",
    "    \n",
    "    # Extract release dates\n",
    "    release_date.str.extract(f\"({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})\", flags = re.IGNORECASE)\n",
    "    \n",
    "    # Use datetime function to parse dates\n",
    "    wiki_movies_df[\"release_date\"] = pd.to_datetime(release_date.str.extract(f\"({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})\")[0], infer_datetime_format = True)\n",
    "    \n",
    "    # Drop original release date column\n",
    "    wiki_movies_df.drop(\"Release date\", axis = 1, inplace = True)\n",
    "    \n",
    "    # Clean running time column in wiki_movies_df\n",
    "    # Make variable to hold non-null running time values and convert lists to strings\n",
    "    running_time = wiki_movies_df[\"Running time\"].dropna().apply(lambda x: \" \".join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Use regex to extract running time values\n",
    "    running_time_extract = running_time.str.extract(r\"(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m\")\n",
    "    \n",
    "    # Convert strings to numeric values and make empty strings 0\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors = \"coerce\")).fillna(0)\n",
    "    \n",
    "    # Convert hour capture groups and minute capture groups to minutes where pure minutes capture group is 0 and save output to wiki_movies_df\n",
    "    wiki_movies_df[\"running_time\"] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis = 1)\n",
    "    \n",
    "    # Drop original running time column\n",
    "    wiki_movies_df.drop(\"Running time\", axis = 1, inplace = True)\n",
    "    \n",
    "    # Clean Kaggle metadata\n",
    "    # Keep only rows where adult column is False and drop adult column\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata[\"adult\"] == \"False\"].drop(\"adult\", axis = \"columns\")\n",
    "\n",
    "    # Convert video column to Boolean data type\n",
    "    kaggle_metadata[\"video\"] = kaggle_metadata[\"video\"] == \"True\"\n",
    "    \n",
    "    # Convert budget, id, and popularity columns to numeric data type\n",
    "    kaggle_metadata[\"budget\"] = kaggle_metadata[\"budget\"].astype(int)\n",
    "    kaggle_metadata[\"id\"] = pd.to_numeric(kaggle_metadata[\"id\"], errors = \"raise\")\n",
    "    kaggle_metadata[\"popularity\"] = pd.to_numeric(kaggle_metadata[\"popularity\"], errors = \"raise\")\n",
    "    \n",
    "    # Convert release date to datetime data type\n",
    "    kaggle_metadata[\"release_date\"] = pd.to_datetime(kaggle_metadata[\"release_date\"])\n",
    "    \n",
    "    # Merge wiki_movies_df and kaggle_metadata\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on = \"imdb_id\", suffixes = [\"_wiki\", \"_kaggle\"])\n",
    "\n",
    "    # Drop redundant Wikipedia columns from merged DF\n",
    "    movies_df.drop(columns = [\"title_wiki\", \"release_date_wiki\", \"Language\", \"Production company(s)\"], inplace = True)\n",
    "\n",
    "    # Add function to fill missing Kaggle data with Wikipedia data and drop Wikipedia columns\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis = 1)\n",
    "        df.drop(columns = wiki_column, inplace = True)\n",
    "\n",
    "    # Call function for 3 columns that need missing data filled\n",
    "    fill_missing_kaggle_data(movies_df, \"runtime\", \"running_time\")\n",
    "    fill_missing_kaggle_data(movies_df, \"budget_kaggle\", \"budget_wiki\")\n",
    "    fill_missing_kaggle_data(movies_df, \"revenue\", \"box_office\")\n",
    "\n",
    "    # Keep and reorder only necessary columns in movies_df\n",
    "    movies_df = movies_df.loc[:, [\"imdb_id\",\"id\",\"title_kaggle\",\"original_title\",\"tagline\",\"belongs_to_collection\",\"url\",\"imdb_link\",\n",
    "                            \"runtime\",\"budget_kaggle\",\"revenue\",\"release_date_kaggle\",\"popularity\",\"vote_average\",\"vote_count\",\n",
    "                            \"genres\",\"original_language\",\"overview\",\"spoken_languages\",\"Country\",\n",
    "                            \"production_companies\",\"production_countries\",\"Distributor\",\n",
    "                            \"Producer(s)\",\"Director\",\"Starring\",\"Cinematography\",\"Editor(s)\",\"Writer(s)\",\"Composer(s)\",\"Based on\"]]\n",
    "\n",
    "    # Rename columns in movies_df for consistency\n",
    "    movies_df.rename({\"id\":\"kaggle_id\",\n",
    "                      \"title_kaggle\":\"title\",\n",
    "                      \"url\":\"wikipedia_url\",\n",
    "                      \"budget_kaggle\":\"budget\",\n",
    "                      \"release_date_kaggle\":\"release_date\",\n",
    "                      \"Country\":\"country\",\n",
    "                      \"Distributor\":\"distributor\",\n",
    "                      \"Producer(s)\":\"producers\",\n",
    "                      \"Director\":\"director\",\n",
    "                      \"Starring\":\"starring\",\n",
    "                      \"Cinematography\":\"cinematography\",\n",
    "                      \"Editor(s)\":\"editors\",\n",
    "                      \"Writer(s)\":\"writers\",\n",
    "                      \"Composer(s)\":\"composers\",\n",
    "                      \"Based on\":\"based_on\"}, \n",
    "                      axis = \"columns\", inplace = True)\n",
    "\n",
    "    # Clean ratings data\n",
    "    # Get count of each rating for each movie\n",
    "    rating_counts = ratings.groupby([\"movieId\", \"rating\"], as_index = False).count() \\\n",
    "                    .rename({\"userId\":\"count\"}, axis = 1) \\\n",
    "                    .pivot(index = \"movieId\", columns = \"rating\", values = \"count\")\n",
    "    \n",
    "    # Add rating_ to beginning of each column name\n",
    "    rating_counts.columns = [\"rating_\" + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    # Merge ratings data into movies_df\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on = \"kaggle_id\", right_index = True, how = \"left\")\n",
    "    \n",
    "    # Fill in missing rating values\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # Add movies_df and MovieLens rating CSV data to SQL database\n",
    "    # Create database engine\n",
    "    engine = create_engine(f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\")\n",
    "    \n",
    "    # Import movies_df to SQL table\n",
    "    movies_df.to_sql(name = \"movies\", con = engine, if_exists = \"replace\")\n",
    "    \n",
    "    # Import ratings to SQL table\n",
    "    # Create variable for number of rows imported\n",
    "    rows_imported = 0\n",
    "    # Get start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(ratings_data, chunksize = 1000000):\n",
    "\n",
    "        # Print range of row being imported\n",
    "        print(f\"importing rows {rows_imported} to {rows_imported + len(data)}...\", end = \"\")\n",
    "\n",
    "        data.to_sql(name = \"ratings\", con = engine, if_exists = \"append\")\n",
    "\n",
    "        # Increment number of rows imported by chunksize\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # Print that rows have finished importing\n",
    "        # Add elapsed time to final print out\n",
    "        print(f\"done: {time.time() - start_time} total seconds elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to file directory and variables for three files \n",
    "file_dir = \"/Users/jenamis/Desktop/BootCamp/Module8/Challenge/movies-etl/Resources\"\n",
    "# Wikipedia data\n",
    "wiki_file = f\"{file_dir}/wikipedia-movies.json\"\n",
    "# Kaggle metadata\n",
    "kaggle_file = f\"{file_dir}/movies_metadata.csv\"\n",
    "# MovieLens rating data\n",
    "ratings_file = f\"{file_dir}/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...done: 18.827893018722534 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...done: 37.71025109291077 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...done: 60.165858030319214 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...done: 78.83414030075073 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...done: 97.58187913894653 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...done: 116.21573138237 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...done: 135.40030121803284 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...done: 154.1742660999298 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...done: 172.79736614227295 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...done: 191.58841705322266 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...done: 209.71172308921814 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...done: 228.1201090812683 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...done: 247.04924321174622 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...done: 265.68567395210266 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...done: 284.26402831077576 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...done: 302.6828360557556 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...done: 321.09308314323425 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...done: 339.6157591342926 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...done: 358.0260360240936 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...done: 376.41033411026 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...done: 394.6704521179199 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...done: 413.6381621360779 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...done: 431.85195231437683 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...done: 451.1028051376343 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...done: 469.99742794036865 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...done: 488.4961280822754 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...done: 488.91851019859314 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Pass three file variables in extract_transform_load function\n",
    "extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
